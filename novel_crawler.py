#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è¯´çˆ¬è™«è„šæœ¬
æ ¹æ®å…³é”®å­—æœç´¢ç¬”è¶£é˜ç½‘ç«™ï¼Œè·å–å°è¯´ç« èŠ‚å†…å®¹å¹¶ä¿å­˜åˆ°txtæ–‡ä»¶
æ”¯æŒå¤šåŸŸåå¹¶è¡Œçˆ¬å–å’Œç« èŠ‚å†…å®¹æ¯”å¯¹æ ¡éªŒ
"""

import json
import requests
import re
import time
import os
import hashlib
from urllib.parse import urljoin, quote, urlparse
from bs4 import BeautifulSoup
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from difflib import SequenceMatcher
from collections import defaultdict
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import sys
from datetime import datetime
import threading


class CrawlDisplay:
    """çˆ¬å–è¿‡ç¨‹ç¾åŒ–æ˜¾ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ˜¾ç¤ºå™¨"""
        self.colors = {
            'red': '\033[91m',
            'green': '\033[92m',
            'yellow': '\033[93m',
            'blue': '\033[94m',
            'purple': '\033[95m',
            'cyan': '\033[96m',
            'white': '\033[97m',
            'bold': '\033[1m',
            'end': '\033[0m'
        }
        self.domain_lines = {}  # å­˜å‚¨æ¯ä¸ªåŸŸåçš„è¡Œå·
        self.total_lines = 0    # æ€»è¡Œæ•°
        self.progress_data = {} # å­˜å‚¨æ¯ä¸ªåŸŸåçš„è¿›åº¦æ•°æ®
        
    def colored_text(self, text, color):
        """è¿”å›å½©è‰²æ–‡æœ¬"""
        if sys.platform == 'win32':
            # Windows ç³»ç»Ÿå¯ç”¨ ANSI é¢œè‰²æ”¯æŒ
            os.system('')
        return f"{self.colors.get(color, '')}{text}{self.colors['end']}"
        
    def print_header(self, title):
        """æ‰“å°æ ‡é¢˜å¤´éƒ¨"""
        border = "â•" * 60
        print(f"\n{self.colored_text(border, 'cyan')}")
        print(f"{self.colored_text(f'  ğŸ“š {title}', 'bold')}")
        print(f"{self.colored_text(border, 'cyan')}\n")
        
    def print_section(self, title):
        """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
        print(f"\n{self.colored_text(f'ğŸ” {title}', 'blue')}")
        print(f"{self.colored_text('â”€' * 40, 'blue')}")
        
    def print_domain_start(self, domain_name, book_title):
        """æ‰“å°åŸŸåå¼€å§‹çˆ¬å–ä¿¡æ¯"""
        print(f"\n{self.colored_text(f'ğŸŒ [{domain_name}]', 'purple')} {self.colored_text('å¼€å§‹çˆ¬å–', 'bold')}: {book_title}")
        
    def print_progress(self, current, total, chapter_title, domain_name):
        """æ‰“å°è¿›åº¦æ¡"""
        percentage = (current / total) * 100
        filled_length = int(30 * current // total)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (30 - filled_length)
        
        # æˆªæ–­ç« èŠ‚æ ‡é¢˜ä»¥é€‚åº”æ˜¾ç¤º
        display_title = chapter_title[:25] + '...' if len(chapter_title) > 25 else chapter_title
        
        print(f"\r{self.colored_text(f'[{domain_name}]', 'purple')} "
              f"{self.colored_text(f'[{bar}]', 'green')} "
              f"{self.colored_text(f'{percentage:5.1f}%', 'yellow')} "
              f"({current}/{total}) {display_title}", end='', flush=True)
        
    def print_chapter_success(self, domain_name, content_length):
        """æ‰“å°ç« èŠ‚æˆåŠŸä¿¡æ¯"""
        print(f" {self.colored_text('âœ“', 'green')} {content_length:,} å­—")
        
    def print_chapter_failed(self, domain_name, reason="è·å–å¤±è´¥"):
        """æ‰“å°ç« èŠ‚å¤±è´¥ä¿¡æ¯"""
        print(f" {self.colored_text('âœ—', 'red')} {reason}")
        
    def print_domain_summary(self, domain_name, success_count, total_count):
        """æ‰“å°åŸŸåçˆ¬å–æ€»ç»“"""
        if success_count > 0:
            print(f"\n{self.colored_text(f'âœ… [{domain_name}]', 'green')} çˆ¬å–å®Œæˆï¼"
                  f"æˆåŠŸè·å– {self.colored_text(str(success_count), 'bold')}/{total_count} ç« ")
        else:
            print(f"\n{self.colored_text(f'âŒ [{domain_name}]', 'red')} çˆ¬å–å¤±è´¥ï¼æœªè·å–åˆ°ä»»ä½•ç« èŠ‚å†…å®¹")
            
    def print_search_results(self, domain_count):
        """æ‰“å°æœç´¢ç»“æœ"""
        print(f"\n{self.colored_text('ğŸ” æœç´¢å®Œæˆ', 'green')}ï¼šåœ¨ {self.colored_text(str(domain_count), 'bold')} ä¸ªåŸŸåä¸­æ‰¾åˆ°ç»“æœ")
        
    def print_crawl_summary(self, successful_domains):
        """æ‰“å°çˆ¬å–æ€»ç»“"""
        print(f"\n{self.colored_text('ğŸ“Š çˆ¬å–æ€»ç»“', 'cyan')}")
        print(f"{self.colored_text('â”€' * 40, 'cyan')}")
        print(f"æˆåŠŸçˆ¬å–çš„åŸŸåæ•°: {self.colored_text(str(len(successful_domains)), 'bold')}")
        for domain in successful_domains:
            domain_name = domain.replace('https://', '').replace('http://', '').replace('www.', '').replace('.', '_')
            print(f"  {self.colored_text('âœ“', 'green')} {domain_name}")
            
    def print_comparison_start(self, novel_title):
        """æ‰“å°æ¯”å¯¹å¼€å§‹ä¿¡æ¯"""
        print(f"\n{self.colored_text('ğŸ”„ å¼€å§‹å†…å®¹æ¯”å¯¹', 'yellow')}: {novel_title}")
        
    def print_merge_start(self, novel_title):
        """æ‰“å°åˆå¹¶å¼€å§‹ä¿¡æ¯"""
        print(f"\n{self.colored_text('ğŸ”§ å¼€å§‹åˆæˆæœ€ä½³ç‰ˆæœ¬', 'blue')}: {novel_title}")
        
    def print_time_info(self, start_time, end_time):
        """æ‰“å°æ—¶é—´ä¿¡æ¯"""
        duration = end_time - start_time
        print(f"\n{self.colored_text('â±ï¸  è€—æ—¶', 'cyan')}: {duration.total_seconds():.1f} ç§’")
        
    def print_title(self, title):
        """æ‰“å°ä¸»æ ‡é¢˜"""
        print(f"\n{self.colored_text('=' * 60, 'cyan')}")
        print(f"{self.colored_text(title, 'bold')}")
        print(f"{self.colored_text('=' * 60, 'cyan')}")
        
    def print_search_result(self, domain_name, result_count):
        """æ‰“å°æœç´¢ç»“æœ"""
        print(f"    âœ… {self.colored_text(domain_name, 'green')} - æ‰¾åˆ° {self.colored_text(str(result_count), 'bold')} ä¸ªç»“æœ")
        
    def print_crawl_summary(self, successful_count):
        """æ‰“å°çˆ¬å–æ€»ç»“"""
        print(f"\n{self.colored_text('ğŸ“Š çˆ¬å–æ€»ç»“', 'cyan')}")
        print(f"{self.colored_text('â”€' * 40, 'cyan')}")
        print(f"  æˆåŠŸçˆ¬å–çš„åŸŸåæ•°: {self.colored_text(str(successful_count), 'bold')}")
        
    def print_compare_start(self):
        """æ‰“å°æ¯”å¯¹å¼€å§‹ä¿¡æ¯"""
        print(f"\n{self.colored_text('ğŸ”„ ç« èŠ‚æ¯”å¯¹é˜¶æ®µ', 'yellow')}")
        print("â”€" * 50)
        
    def print_merge_start(self):
        """æ‰“å°åˆå¹¶å¼€å§‹ä¿¡æ¯"""
        print(f"\n{self.colored_text('ğŸ”§ æœ€ä½³ç‰ˆæœ¬åˆæˆé˜¶æ®µ', 'blue')}")
        print("â”€" * 50)
        
    def init_multi_domain_progress(self, domains):
        """åˆå§‹åŒ–å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
        
        Args:
            domains (list): åŸŸååˆ—è¡¨
        """
        self.progress_data = {}
        self.last_update_time = 0
        self.progress_lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”
        self.display_initialized = False
        
        print(f"\n{self.colored_text('ğŸ“š å¹¶å‘çˆ¬å–è¿›åº¦', 'yellow')}")
        print("â”€" * 80)
        
        # åˆå§‹åŒ–è¿›åº¦æ•°æ®
        for domain in sorted(domains):  # æ’åºç¡®ä¿é¡ºåºä¸€è‡´
            domain_name = domain.replace('https://', '').replace('http://', '').replace('www.', '').split('.')[0]
            self.progress_data[domain] = {
                'name': domain_name,
                'current': 0,
                'total': 0,
                'chapter_title': 'å‡†å¤‡ä¸­...',
                'status': 'waiting'
            }
        
        self.display_initialized = True
        print(f"  åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹çˆ¬å– {len(domains)} ä¸ªåŸŸå...\n")
        
    def update_domain_progress(self, domain, current, total, chapter_title, status='running'):
        """æ›´æ–°æŒ‡å®šåŸŸåçš„è¿›åº¦
        
        Args:
            domain (str): åŸŸå
            current (int): å½“å‰ç« èŠ‚æ•°
            total (int): æ€»ç« èŠ‚æ•°
            chapter_title (str): å½“å‰ç« èŠ‚æ ‡é¢˜
            status (str): çŠ¶æ€ ('running', 'success', 'failed')
        """
        if domain not in self.progress_data:
            return
            
        with self.progress_lock:  # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤
            # æ›´æ–°è¿›åº¦æ•°æ®
            self.progress_data[domain].update({
                'current': current,
                'total': total,
                'chapter_title': chapter_title[:30],  # é™åˆ¶é•¿åº¦
                'status': status
            })
            
            # é™åˆ¶æ›´æ–°é¢‘ç‡ï¼Œé¿å…é—ªçƒ
            import time
            current_time = time.time()
            
            # æ¯2ç§’æˆ–è€…é‡è¦çŠ¶æ€å˜åŒ–æ—¶æ˜¾ç¤ºè¿›åº¦
            if (current_time - self.last_update_time > 2.0 or 
                status in ['success', 'failed']):
                self._show_progress_summary()
                self.last_update_time = current_time
        
    def _show_progress_summary(self):
        """æ˜¾ç¤ºè¿›åº¦æ±‡æ€»"""
        if not self.display_initialized:
            return
            
        # ç»Ÿè®¡å„çŠ¶æ€çš„åŸŸåæ•°é‡
        status_counts = {'waiting': 0, 'running': 0, 'success': 0, 'failed': 0}
        total_progress = 0
        active_domains = []
        
        for domain, data in self.progress_data.items():
            status_counts[data['status']] += 1
            if data['total'] > 0:
                progress = (data['current'] / data['total']) * 100
                total_progress += progress
                
            if data['status'] == 'running' and data['current'] > 0:
                active_domains.append(f"{data['name']}({data['current']}/{data['total']})")
        
        avg_progress = total_progress / len(self.progress_data) if self.progress_data else 0
        
        # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
        print(f"\r  ğŸ“Š è¿›åº¦æ±‡æ€»: å¹³å‡ {avg_progress:.1f}% | "
              f"è¿è¡Œä¸­: {self.colored_text(str(status_counts['running']), 'cyan')} | "
              f"å®Œæˆ: {self.colored_text(str(status_counts['success']), 'green')} | "
              f"å¤±è´¥: {self.colored_text(str(status_counts['failed']), 'red')} | "
              f"ç­‰å¾…: {status_counts['waiting']}", end='', flush=True)
        
        # å¦‚æœæœ‰æ´»è·ƒåŸŸåï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if active_domains and len(active_domains) <= 3:
            print(f" | æ´»è·ƒ: {', '.join(active_domains[:3])}", end='', flush=True)
        
    def _format_progress_line(self, data):
        """æ ¼å¼åŒ–å•è¡Œè¿›åº¦æ˜¾ç¤º
        
        Args:
            data (dict): è¿›åº¦æ•°æ®
            
        Returns:
            str: æ ¼å¼åŒ–çš„è¿›åº¦è¡Œ
        """
        # è®¡ç®—è¿›åº¦
        if data['total'] > 0:
            percentage = (data['current'] / data['total']) * 100
            filled_length = int(30 * data['current'] // data['total'])
        else:
            percentage = 0
            filled_length = 0
            
        # ç”Ÿæˆè¿›åº¦æ¡
        bar = 'â–ˆ' * filled_length + 'â–‘' * (30 - filled_length)
        
        # æ ¹æ®çŠ¶æ€é€‰æ‹©é¢œè‰²
        if data['status'] == 'success':
            bar_color = 'green'
            name_color = 'green'
        elif data['status'] == 'failed':
            bar_color = 'red'
            name_color = 'red'
        elif data['status'] == 'running':
            bar_color = 'blue'
            name_color = 'cyan'
        else:  # waiting
            bar_color = 'white'
            name_color = 'white'
        
        # æ ¼å¼åŒ–è¾“å‡º
        domain_name = self.colored_text(f"{data['name']:<12}", name_color)
        progress_bar = self.colored_text(bar, bar_color)
        percentage_text = self.colored_text(f"{percentage:>6.1f}%", 'yellow')
        count_text = f"{data['current']:>3}/{data['total']:<3}"
        chapter_text = data['chapter_title'][:35]  # é™åˆ¶ç« èŠ‚åé•¿åº¦
        
        return f"{domain_name} {progress_bar} {percentage_text} {count_text} {chapter_text}"
    

            
    def finish_domain_progress(self, domain, success_count, total_count):
        """å®ŒæˆåŸŸåè¿›åº¦æ˜¾ç¤º
        
        Args:
            domain (str): åŸŸå
            success_count (int): æˆåŠŸç« èŠ‚æ•°
            total_count (int): æ€»ç« èŠ‚æ•°
        """
        if success_count > 0:
            status_text = f"å®Œæˆ! æˆåŠŸ {success_count}/{total_count} ç« "
            self.update_domain_progress(domain, success_count, total_count, status_text, 'success')
        else:
            status_text = "å¤±è´¥! æœªè·å–åˆ°å†…å®¹"
            self.update_domain_progress(domain, 0, total_count, status_text, 'failed')
            
    def finalize_progress_display(self):
        """å®Œæˆæ‰€æœ‰è¿›åº¦æ˜¾ç¤ºï¼Œç§»åŠ¨å…‰æ ‡åˆ°æœ€å"""
        if self.display_initialized:
            # æ˜¾ç¤ºæœ€ç»ˆè¯¦ç»†ç»“æœ
            print("\n\nğŸ“‹ æœ€ç»ˆç»“æœè¯¦æƒ…:")
            print("â”€" * 60)
            
            for domain in sorted(self.progress_data.keys()):
                data = self.progress_data[domain]
                line = self._format_progress_line(data)
                print(f"  {line}")
            
            print()  # æ·»åŠ ä¸€ä¸ªç©ºè¡Œåˆ†éš”


class NovelCrawler:
    """å°è¯´çˆ¬è™«ç±»"""
    
    def __init__(self, domains_file='all_domains.json', output_dir='novel_output', use_selenium=True, reference_sources=None):
        """åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            domains_file (str): åŸŸåé…ç½®æ–‡ä»¶è·¯å¾„
            output_dir (str): è¾“å‡ºç›®å½•
            use_selenium (bool): æ˜¯å¦ä½¿ç”¨Selenium
            reference_sources (list): åŸºå‡†æºåˆ—è¡¨ï¼Œç”¨äºå†…å®¹åˆå¹¶æ—¶çš„ä¼˜å…ˆçº§è®¾ç½®
        """
        self.domains_file = domains_file
        self.output_dir = output_dir
        self.domains = self.load_domains()
        self.session = requests.Session()
        self.use_selenium = use_selenium
        self.driver = None
        self.display = CrawlDisplay()  # æ·»åŠ ç¾åŒ–æ˜¾ç¤ºå™¨
        
        # è®¾ç½®åŸºå‡†æºä¼˜å…ˆçº§ï¼ˆé»˜è®¤å€¼ï¼‰
        self.reference_sources = reference_sources or ['bqgam', 'biquge', '675m', 'bqg67', 'biqu10']
        
        # åˆå¹¶ç­–ç•¥é…ç½®
        self.merge_config = {
            'enable_diff_merge': True,  # æ˜¯å¦å¯ç”¨å·®åˆ†åˆå¹¶
            'length_priority_weight': 0.6,  # é•¿åº¦ä¼˜å…ˆæƒé‡
            'quality_weight': 0.4,  # è´¨é‡è¯„ä¼°æƒé‡
            'min_content_length': 200,  # æœ€å°å†…å®¹é•¿åº¦
            'merge_threshold': 1.1,  # åˆå¹¶é˜ˆå€¼ï¼ˆåˆå¹¶åå†…å®¹å¢é•¿æ¯”ä¾‹ï¼‰
            'similarity_threshold': 0.8  # ç›¸ä¼¼åº¦é˜ˆå€¼
        }
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        if use_selenium:
            self._init_selenium()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def configure_merge_strategy(self, **kwargs):
        """é…ç½®åˆå¹¶ç­–ç•¥å‚æ•°
        
        Args:
            **kwargs: åˆå¹¶ç­–ç•¥é…ç½®å‚æ•°
                - reference_sources: åŸºå‡†æºåˆ—è¡¨
                - enable_diff_merge: æ˜¯å¦å¯ç”¨å·®åˆ†åˆå¹¶
                - length_priority_weight: é•¿åº¦ä¼˜å…ˆæƒé‡
                - quality_weight: è´¨é‡è¯„ä¼°æƒé‡
                - min_content_length: æœ€å°å†…å®¹é•¿åº¦
                - merge_threshold: åˆå¹¶é˜ˆå€¼
                - similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        if 'reference_sources' in kwargs:
            self.reference_sources = kwargs['reference_sources']
            
        for key, value in kwargs.items():
            if key in self.merge_config:
                self.merge_config[key] = value
                
        print(f"åˆå¹¶ç­–ç•¥å·²æ›´æ–°: {kwargs}")
    
    def load_domains(self):
        """åŠ è½½åŸŸåé…ç½®æ–‡ä»¶
        
        Returns:
            list: åŸŸååˆ—è¡¨
        """
        try:
            with open(self.domains_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('domains', [])
        except Exception as e:
            print(f"åŠ è½½åŸŸåé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def search_novel_single_domain(self, domain, keyword):
        """åœ¨å•ä¸ªåŸŸåä¸­æœç´¢å°è¯´
        
        Args:
            domain (str): åŸŸå
            keyword (str): æœç´¢å…³é”®å­—
            
        Returns:
            tuple: (åŸŸå, æœç´¢ç»“æœåˆ—è¡¨)
        """
        try:
            # æ„å»ºæœç´¢URL
            search_url = f"{domain.rstrip('/')}/s?q={quote(keyword)}"
            print(f"å°è¯•æœç´¢: {search_url}")
            
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()
            
            # è§£ææœç´¢ç»“æœ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æœç´¢ç»“æœçš„JavaScriptä»£ç 
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'loadmore' in script.string:
                    # å°è¯•è·å–æœç´¢API
                    api_match = re.search(r'\$.getJSON\("([^"]+)"', script.string)
                    if api_match:
                        api_url = api_match.group(1)
                        if not api_url.startswith('http'):
                            api_url = urljoin(domain, api_url)
                        
                        # ç¡®ä¿API URLåŒ…å«æœç´¢å…³é”®å­—
                        if '?q=' in api_url and not api_url.endswith('?q='):
                            # API URLå·²ç»åŒ…å«å®Œæ•´çš„æŸ¥è¯¢å‚æ•°
                            pass
                        elif '?q=' in api_url and api_url.endswith('?q='):
                            # API URLç¼ºå°‘æŸ¥è¯¢å…³é”®å­—ï¼Œæ·»åŠ å…³é”®å­—
                            api_url += quote(keyword)
                        else:
                            # API URLä¸åŒ…å«æŸ¥è¯¢å‚æ•°ï¼Œæ·»åŠ å®Œæ•´çš„æŸ¥è¯¢å‚æ•°
                            separator = '&' if '?' in api_url else '?'
                            api_url += f"{separator}q={quote(keyword)}"
                        
                        # å…ˆè°ƒç”¨hm.html APIï¼ˆæ ¹æ®ç½‘ç«™çš„æœç´¢é€»è¾‘ï¼‰
                        hm_url = urljoin(domain, f"/user/hm.html?q={quote(keyword)}")
                        try:
                            hm_response = self.session.get(hm_url, timeout=10)
                        except:
                            pass  # hm.htmlå¯èƒ½ä¸å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œ
                        
                        # è°ƒç”¨æœç´¢API
                        api_response = self.session.get(api_url, timeout=10)
                        if api_response.status_code == 200:
                            try:
                                search_results = api_response.json()
                                if search_results and isinstance(search_results, list) and len(search_results) > 0:
                                    print(f"åœ¨ {domain} æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                                    return domain, search_results
                            except json.JSONDecodeError:
                                continue
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"æœç´¢ {domain} å¤±è´¥: {e}")
        
        return domain, []
    
    def search_novel_all_domains(self, keyword, max_workers=3):
        """åœ¨æ‰€æœ‰åŸŸåä¸­å¹¶è¡Œæœç´¢å°è¯´
        
        Args:
            keyword (str): æœç´¢å…³é”®å­—
            max_workers (int): æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            dict: {åŸŸå: æœç´¢ç»“æœåˆ—è¡¨}
        """
        results = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_domain = {
                executor.submit(self.search_novel_single_domain, domain, keyword): domain 
                for domain in self.domains
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_domain):
                domain, search_results = future.result()
                domain_name = self.get_domain_name(domain)
                if search_results:
                    results[domain] = search_results
                    self.display.print_search_result(domain_name, len(search_results))
                else:
                    print(f"    âŒ {domain_name} - æœªæ‰¾åˆ°ç»“æœ")
        
        return results
    
    def get_chapter_list(self, domain, book_url):
        """è·å–ç« èŠ‚åˆ—è¡¨
        
        Args:
            domain (str): åŸŸå
            book_url (str): å°è¯´è¯¦æƒ…é¡µURL
            
        Returns:
            list: ç« èŠ‚é“¾æ¥åˆ—è¡¨
        """
        try:
            # æ„å»ºå®Œæ•´URL
            if not book_url.startswith('http'):
                full_url = urljoin(domain, book_url)
            else:
                full_url = book_url
            
            print(f"è·å–ç« èŠ‚åˆ—è¡¨: {full_url}")
            
            response = self.session.get(full_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾ç« èŠ‚åˆ—è¡¨ - æ”¯æŒå¤šç§é“¾æ¥æ¨¡å¼
            chapters = []
            # å°è¯•å¤šç§ç« èŠ‚é“¾æ¥æ¨¡å¼
            patterns = [
                r'/index/\d+/\d+\.html',  # bqgam.com æ¨¡å¼
                r'/book/\d+/\d+\.html',   # 675m.com æ¨¡å¼
                r'/\d+/\d+\.html',        # é€šç”¨æ¨¡å¼
            ]
            
            for pattern in patterns:
                chapter_links = soup.find_all('a', href=re.compile(pattern))
                if chapter_links:
                    break
            
            for link in chapter_links:
                href = link.get('href')
                title = link.get_text().strip()
                if href and title:
                    chapters.append({
                        'title': title,
                        'url': href
                    })
            
            print(f"æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚")
            return chapters
            
        except Exception as e:
            print(f"è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_chapter_content(self, domain, chapter_url, thread_driver=None):
        """è·å–ç« èŠ‚å†…å®¹
        
        Args:
            domain (str): åŸŸå
            chapter_url (str): ç« èŠ‚URL
            thread_driver (webdriver.Chrome, optional): çº¿ç¨‹ä¸“ç”¨çš„WebDriverå®ä¾‹
            
        Returns:
            tuple: (ç« èŠ‚æ ‡é¢˜, ç« èŠ‚å†…å®¹)
        """
        try:
            # æ„å»ºå®Œæ•´URL
            if not chapter_url.startswith('http'):
                full_chapter_url = urljoin(domain, chapter_url)
            else:
                full_chapter_url = chapter_url
            
            # æ–¹æ³•1: å°è¯•geturl API
            try:
                geturl_api = f"{domain.rstrip('/')}/user/geturl.html?url={full_chapter_url}"
                response = self.session.get(geturl_api, timeout=10, allow_redirects=False)
                
                if response.status_code in [301, 302]:
                    real_url = response.headers.get('Location')
                    if real_url:
                        content_response = self.session.get(real_url, timeout=10)
                        if content_response.status_code == 200:
                            title, content = self._extract_content_from_html(content_response.text)
                            if title and content and len(content) > 50:  # ç¡®ä¿å†…å®¹ä¸æ˜¯"åŠ è½½ä¸­"
                                return title, content
            except:
                pass
            
            # æ–¹æ³•2: ä½¿ç”¨Seleniumï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_selenium:
                # ä¼˜å…ˆä½¿ç”¨çº¿ç¨‹ä¸“ç”¨çš„WebDriver
                if thread_driver:
                    title, content = self.get_chapter_content_selenium(domain, chapter_url, thread_driver)
                    if title and content:
                        return title, content
                else:
                    # æ£€æŸ¥WebDriverçŠ¶æ€ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é‡æ–°åˆå§‹åŒ–
                    if not self.driver:
                        self._init_selenium()
                    
                    if self.driver:
                        title, content = self.get_chapter_content_selenium(domain, chapter_url)
                        if title and content:
                            return title, content
            
            # æ–¹æ³•3: ç›´æ¥è®¿é—®ç« èŠ‚é¡µé¢
            try:
                direct_response = self.session.get(full_chapter_url, timeout=10)
                if direct_response.status_code == 200:
                    title, content = self._extract_content_from_html(direct_response.text)
                    if title and content and len(content) > 50:
                        return title, content
            except:
                pass
            
            # æ–¹æ³•4: å°è¯•å…¶ä»–å¯èƒ½çš„APIç«¯ç‚¹
            try:
                # æœ‰äº›ç½‘ç«™ä½¿ç”¨ä¸åŒçš„API
                alt_apis = [
                    f"{domain.rstrip('/')}/user/book_read.html?bid={chapter_url.split('/')[-2]}&cid={chapter_url.split('/')[-1].replace('.html', '')}",
                    f"{domain.rstrip('/')}/read{chapter_url}",
                    f"{domain.rstrip('/')}/chapter{chapter_url}"
                ]
                
                for api_url in alt_apis:
                    try:
                        api_response = self.session.get(api_url, timeout=10)
                        if api_response.status_code == 200:
                            title, content = self._extract_content_from_html(api_response.text)
                            if title and content and len(content) > 50:
                                return title, content
                    except:
                        continue
            except:
                pass
            
            return None, None
            
        except Exception as e:
            print(f"è·å–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return None, None
    
    def _init_selenium(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        try:
            # å¦‚æœå·²æœ‰driverï¼Œå…ˆå…³é—­
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
            
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
            # chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä½¿ç”¨webdriver-managerè‡ªåŠ¨ç®¡ç†é©±åŠ¨ç¨‹åº
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            print("Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"Selenium WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            self.driver = None
            return False
    
    def _create_thread_driver(self):
        """ä¸ºå½“å‰çº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„WebDriverå®ä¾‹
        
        Returns:
            webdriver.Chrome: Chrome WebDriverå®ä¾‹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä½¿ç”¨webdriver-managerè‡ªåŠ¨ç®¡ç†é©±åŠ¨ç¨‹åº
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            return driver
        except Exception as e:
            print(f"åˆ›å»ºçº¿ç¨‹WebDriverå¤±è´¥: {e}")
            return None
    
    def get_chapter_content_selenium(self, domain, chapter_url, thread_driver=None):
        """ä½¿ç”¨Seleniumè·å–ç« èŠ‚å†…å®¹
        
        Args:
            domain (str): åŸŸå
            chapter_url (str): ç« èŠ‚URL
            thread_driver (webdriver.Chrome, optional): çº¿ç¨‹ä¸“ç”¨çš„WebDriverå®ä¾‹
            
        Returns:
            tuple: (ç« èŠ‚æ ‡é¢˜, ç« èŠ‚å†…å®¹)
        """
        # ä½¿ç”¨ä¼ å…¥çš„çº¿ç¨‹driveræˆ–è€…å®ä¾‹çš„driver
        driver = thread_driver if thread_driver else self.driver
        if not driver:
            return None, None
            
        try:
            # æ„å»ºå®Œæ•´URL
            if not chapter_url.startswith('http'):
                full_chapter_url = urljoin(domain, chapter_url)
            else:
                full_chapter_url = chapter_url
            
            # è®¿é—®é¡µé¢
            driver.get(full_chapter_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(2)  # å‡å°‘ç­‰å¾…æ—¶é—´ä»¥æé«˜å¹¶å‘æ•ˆç‡
            
            # ç­‰å¾…å†…å®¹åŠ è½½å®Œæˆ
            try:
                WebDriverWait(driver, 8).until(
                    lambda d: d.title != "åŠ è½½ä¸­â€¦â€¦" and len(d.page_source) > 2000
                )
            except TimeoutException:
                pass  # ç»§ç»­å¤„ç†ï¼Œä¸æ‰“å°é”™è¯¯ä¿¡æ¯é¿å…å¹¶å‘æ—¶è¾“å‡ºæ··ä¹±
            
            # è·å–é¡µé¢æºç 
            page_source = driver.page_source
            
            # æå–æ ‡é¢˜å’Œå†…å®¹
            title, content = self._extract_content_from_html(page_source)
            
            if title and content and len(content) > 50:
                return title, content
            
            return None, None
            
        except Exception as e:
            # åœ¨å¹¶å‘æ¨¡å¼ä¸‹å‡å°‘é”™è¯¯è¾“å‡º
            return None, None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'driver') and self.driver:
            try:
                self.driver.quit()
                self.driver = None
                print("WebDriverå·²å…³é—­")
            except Exception as e:
                print(f"å…³é—­WebDriveræ—¶å‡ºé”™: {e}")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        self.cleanup()
    
    def _extract_content_from_html(self, html_text):
        """
        ä»HTMLä¸­æå–ç« èŠ‚æ ‡é¢˜å’Œå†…å®¹
        
        Args:
            html_text (str): HTMLæ–‡æœ¬
            
        Returns:
            tuple: (æ ‡é¢˜, å†…å®¹)
        """
        try:
            soup = BeautifulSoup(html_text, 'html.parser')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯"åŠ è½½ä¸­"é¡µé¢
            if "åŠ è½½ä¸­" in html_text or len(html_text) < 2000:
                return None, None
            
            # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
            title = None
            title_selectors = [
                'h1.wap_none',
                'h1',
                '.title',
                '.chapter-title',
                '.book-title',
                'h2',
                'h3'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    if title_text and title_text != "åŠ è½½ä¸­â€¦â€¦" and len(title_text) > 2:
                        title = title_text
                        break
            
            # æå–å†…å®¹ - å°è¯•å¤šç§é€‰æ‹©å™¨
            content = None
            content_selectors = [
                '#chaptercontent',
                '.content',
                '.chapter-content',
                '.text-content',
                '#content',
                '.txt',
                '.book-content',
                '.read-content',
                'div[class*="content"]',
                'div[id*="content"]'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content_text = content_elem.get_text().strip()
                    if content_text and len(content_text) > 50:  # ç¡®ä¿æœ‰å®é™…å†…å®¹
                        # æ¸…ç†å†…å®¹
                        content = re.sub(r'\n\s*\n', '\n\n', content_text)
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸“é—¨çš„å†…å®¹åŒºåŸŸï¼Œå°è¯•ä»bodyä¸­æå–
            if not content:
                body = soup.find('body')
                if body:
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                    for script in body(["script", "style"]):
                        script.decompose()
                    
                    body_text = body.get_text().strip()
                    if len(body_text) > 200:
                        content = re.sub(r'\n\s*\n', '\n\n', body_text)
            
            return title or "æœªçŸ¥ç« èŠ‚", content
            
        except Exception as e:
            print(f"è§£æHTMLå¤±è´¥: {e}")
            return None, None
    
    def get_domain_name(self, domain):
        """ä»åŸŸåURLä¸­æå–åŸŸååç§°
        
        Args:
            domain (str): å®Œæ•´åŸŸåURL
            
        Returns:
            str: åŸŸååç§°
        """
        parsed = urlparse(domain)
        return parsed.netloc.replace('www.', '').replace('.', '_')
    
    def save_novel_by_domain(self, novel_title, domain, chapters_data):
        """æŒ‰åŸŸåä¿å­˜å°è¯´åˆ°ä¸åŒæ–‡ä»¶å¤¹
        
        Args:
            novel_title (str): å°è¯´åç§°
            domain (str): åŸŸå
            chapters_data (list): ç« èŠ‚æ•°æ®åˆ—è¡¨
        """
        domain_name = self.get_domain_name(domain)
        # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
        domain_dir = os.path.join(self.output_dir, safe_title, domain_name)
        
        # åˆ›å»ºåŸŸåç›®å½•
        if not os.path.exists(domain_dir):
            os.makedirs(domain_dir)
        
        filename = os.path.join(domain_dir, f"{safe_title}.txt")
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                for chapter_title, chapter_content in chapters_data:
                    if chapter_title and chapter_content:
                        f.write(f"{chapter_title}\n")
                        f.write("=" * 50 + "\n")
                        f.write(f"{chapter_content}\n\n")
            
            print(f"å°è¯´å·²ä¿å­˜åˆ°: {filename}")
            
            # åŒæ—¶ä¿å­˜ç« èŠ‚æ•°æ®ä¸ºJSONæ ¼å¼ï¼Œä¾¿äºåç»­æ¯”å¯¹
            json_filename = os.path.join(domain_dir, f"{safe_title}_chapters.json")
            chapters_json = []
            for chapter_title, chapter_content in chapters_data:
                if chapter_title and chapter_content:
                    chapters_json.append({
                        'title': chapter_title,
                        'content': chapter_content,
                        'content_hash': hashlib.md5(chapter_content.encode('utf-8')).hexdigest()
                    })
            
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(chapters_json, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1 (str): æ–‡æœ¬1
            text2 (str): æ–‡æœ¬2
            
        Returns:
            float: ç›¸ä¼¼åº¦ (0-1)
        """
        return SequenceMatcher(None, text1, text2).ratio()
    
    def compare_chapters(self, novel_title):
        """æ¯”å¯¹ä¸åŒåŸŸåçš„ç« èŠ‚å†…å®¹
        
        Args:
            novel_title (str): å°è¯´åç§°
        """
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
        novel_dir = os.path.join(self.output_dir, safe_title)
        if not os.path.exists(novel_dir):
            print(f"æœªæ‰¾åˆ°å°è¯´ç›®å½•: {novel_dir}")
            return
        
        # æ”¶é›†æ‰€æœ‰åŸŸåçš„ç« èŠ‚æ•°æ®
        domain_chapters = {}
        for domain_name in os.listdir(novel_dir):
            domain_path = os.path.join(novel_dir, domain_name)
            if os.path.isdir(domain_path):
                json_file = os.path.join(domain_path, f"{safe_title}_chapters.json")
                if os.path.exists(json_file):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            chapters = json.load(f)
                            domain_chapters[domain_name] = chapters
                    except Exception as e:
                        print(f"è¯»å– {json_file} å¤±è´¥: {e}")
        
        if len(domain_chapters) < 2:
            print("éœ€è¦è‡³å°‘ä¸¤ä¸ªåŸŸåçš„æ•°æ®æ‰èƒ½è¿›è¡Œæ¯”å¯¹")
            return
        
        # ç”Ÿæˆæ¯”å¯¹æŠ¥å‘Š
        comparison_report = {
            'novel_title': novel_title,
            'domains': list(domain_chapters.keys()),
            'comparison_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'chapter_comparison': []
        }
        
        # æŒ‰ç« èŠ‚æ ‡é¢˜åˆ†ç»„
        chapters_by_title = defaultdict(dict)
        for domain_name, chapters in domain_chapters.items():
            for chapter in chapters:
                title = chapter['title']
                chapters_by_title[title][domain_name] = chapter
        
        # æ¯”å¯¹æ¯ä¸ªç« èŠ‚
        for title, domain_data in chapters_by_title.items():
            if len(domain_data) < 2:
                continue  # è·³è¿‡åªæœ‰ä¸€ä¸ªåŸŸåçš„ç« èŠ‚
            
            chapter_report = {
                'title': title,
                'domains_count': len(domain_data),
                'similarities': [],
                'content_hashes': {}
            }
            
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            for domain_name, chapter_data in domain_data.items():
                chapter_report['content_hashes'][domain_name] = chapter_data['content_hash']
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            domain_names = list(domain_data.keys())
            for i in range(len(domain_names)):
                for j in range(i + 1, len(domain_names)):
                    domain1, domain2 = domain_names[i], domain_names[j]
                    content1 = domain_data[domain1]['content']
                    content2 = domain_data[domain2]['content']
                    
                    similarity = self.calculate_similarity(content1, content2)
                    chapter_report['similarities'].append({
                        'domain1': domain1,
                        'domain2': domain2,
                        'similarity': similarity
                    })
            
            comparison_report['chapter_comparison'].append(chapter_report)
        
        # ä¿å­˜æ¯”å¯¹æŠ¥å‘Š
        report_file = os.path.join(novel_dir, 'comparison_report.json')
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_report, f, ensure_ascii=False, indent=2)
            print(f"æ¯”å¯¹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            # ç”Ÿæˆç®€è¦æ‘˜è¦
            self.generate_comparison_summary(comparison_report, novel_dir)
            
        except Exception as e:
            print(f"ä¿å­˜æ¯”å¯¹æŠ¥å‘Šå¤±è´¥: {e}")
    
    def generate_comparison_summary(self, comparison_report, output_dir):
        """ç”Ÿæˆæ¯”å¯¹æ‘˜è¦
        
        Args:
            comparison_report (dict): æ¯”å¯¹æŠ¥å‘Š
            output_dir (str): è¾“å‡ºç›®å½•
        """
        summary_file = os.path.join(output_dir, 'comparison_summary.txt')
        
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"ç« èŠ‚å†…å®¹æ¯”å¯¹æ‘˜è¦\n")
                f.write(f"å°è¯´åç§°: {comparison_report['novel_title']}\n")
                f.write(f"æ¯”å¯¹æ—¶é—´: {comparison_report['comparison_time']}\n")
                f.write(f"å‚ä¸æ¯”å¯¹çš„åŸŸå: {', '.join(comparison_report['domains'])}\n")
                f.write("=" * 60 + "\n\n")
                
                total_chapters = len(comparison_report['chapter_comparison'])
                high_similarity_count = 0
                low_similarity_count = 0
                
                for chapter in comparison_report['chapter_comparison']:
                    f.write(f"ç« èŠ‚: {chapter['title']}\n")
                    f.write(f"åŸŸåæ•°é‡: {chapter['domains_count']}\n")
                    
                    avg_similarity = 0
                    if chapter['similarities']:
                        similarities = [s['similarity'] for s in chapter['similarities']]
                        avg_similarity = sum(similarities) / len(similarities)
                        
                        if avg_similarity >= 0.8:
                            high_similarity_count += 1
                            f.write(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f} (é«˜)\n")
                        elif avg_similarity >= 0.5:
                            f.write(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f} (ä¸­)\n")
                        else:
                            low_similarity_count += 1
                            f.write(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f} (ä½)\n")
                        
                        for sim in chapter['similarities']:
                            f.write(f"  {sim['domain1']} vs {sim['domain2']}: {sim['similarity']:.3f}\n")
                    
                    f.write("\n")
                
                f.write("=" * 60 + "\n")
                f.write(f"æ€»ç« èŠ‚æ•°: {total_chapters}\n")
                f.write(f"é«˜ç›¸ä¼¼åº¦ç« èŠ‚ (>=0.8): {high_similarity_count}\n")
                f.write(f"ä½ç›¸ä¼¼åº¦ç« èŠ‚ (<0.5): {low_similarity_count}\n")
                
            print(f"æ¯”å¯¹æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
            
        except Exception as e:
            print(f"ç”Ÿæˆæ¯”å¯¹æ‘˜è¦å¤±è´¥: {e}")
    
    def crawl_novel_single_domain_with_selenium(self, keyword, domain, search_results, max_chapters=None, use_multi_progress=False):
        """ä½¿ç”¨Seleniumçˆ¬å–å•ä¸ªåŸŸåçš„å°è¯´ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
        
        Args:
            keyword (str): æœç´¢å…³é”®å­—
            domain (str): åŸŸå
            search_results (list): æœç´¢ç»“æœåˆ—è¡¨
            max_chapters (int, optional): æœ€å¤§ç« èŠ‚æ•°é™åˆ¶
            use_multi_progress (bool): æ˜¯å¦ä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
            
        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, å°è¯´æ ‡é¢˜)
        """
        domain_name = self.get_domain_name(domain)
        thread_driver = None
        
        try:
            # ä¸ºå½“å‰çº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„WebDriver
            thread_driver = self._create_thread_driver()
            if not thread_driver:
                if use_multi_progress:
                    self.display.update_domain_progress(domain, 0, 0, "WebDriveråˆ›å»ºå¤±è´¥", 'failed')
                return False, None
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
            first_result = search_results[0]
            book_url = first_result.get('url_list')
            book_title = first_result.get('articlename', keyword)
            
            # å¦‚æœä¸ä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤ºï¼Œåˆ™ä½¿ç”¨åŸæœ‰çš„æ˜¾ç¤ºæ–¹å¼
            if not use_multi_progress:
                self.display.print_domain_start(domain_name, book_title)
            
            # è·å–ç« èŠ‚åˆ—è¡¨
            chapters = self.get_chapter_list(domain, book_url)
            if not chapters:
                if use_multi_progress:
                    self.display.finish_domain_progress(domain, 0, 0)
                else:
                    self.display.print_domain_summary(domain_name, 0, 0)
                return False, None
            
            # é™åˆ¶ç« èŠ‚æ•°é‡
            if max_chapters and len(chapters) > max_chapters:
                chapters = chapters[:max_chapters]
                if not use_multi_progress:
                    print(f"  ğŸ“Š é™åˆ¶çˆ¬å–å‰ {max_chapters} ç« ï¼ˆå…± {len(chapters)} ç« å¯ç”¨ï¼‰")
            
            # åˆå§‹åŒ–è¿›åº¦ï¼ˆå¦‚æœä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤ºï¼‰
            if use_multi_progress:
                self.display.update_domain_progress(domain, 0, len(chapters), "å¼€å§‹çˆ¬å–...", 'running')
            
            # çˆ¬å–ç« èŠ‚å†…å®¹
            chapters_data = []
            for i, chapter in enumerate(chapters, 1):
                if use_multi_progress:
                    # æ›´æ–°å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
                    self.display.update_domain_progress(domain, i, len(chapters), chapter['title'], 'running')
                else:
                    # ä½¿ç”¨åŸæœ‰çš„è¿›åº¦æ˜¾ç¤º
                    self.display.print_progress(i, len(chapters), chapter['title'], domain_name)
                
                # ä½¿ç”¨çº¿ç¨‹ä¸“ç”¨çš„WebDriver
                title, content = self.get_chapter_content(domain, chapter['url'], thread_driver)
                
                # æ ¹æ®ç»“æœå¤„ç†
                if title and content and len(content) > 50:
                    chapters_data.append((title, content))
                    if not use_multi_progress:
                        self.display.print_chapter_success(domain_name, len(content))
                else:
                    if not use_multi_progress:
                        reason = "å†…å®¹ä¸ºç©º" if not content else f"å†…å®¹è¿‡çŸ­({len(content) if content else 0}å­—)"
                        self.display.print_chapter_failed(domain_name, reason)
                
                # æ·»åŠ å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)  # å‡å°‘å»¶æ—¶ä»¥æé«˜å¹¶å‘æ•ˆç‡
            
            # å®Œæˆè¿›åº¦æ˜¾ç¤º
            if use_multi_progress:
                self.display.finish_domain_progress(domain, len(chapters_data), len(chapters))
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            if chapters_data:
                self.save_novel_by_domain(book_title, domain, chapters_data)
                if not use_multi_progress:
                    self.display.print_domain_summary(domain_name, len(chapters_data), len(chapters))
                return True, book_title
            else:
                if not use_multi_progress:
                    self.display.print_domain_summary(domain_name, 0, len(chapters))
                return False, None
                
        except Exception as e:
            if use_multi_progress:
                self.display.update_domain_progress(domain, 0, 0, f"é”™è¯¯: {str(e)[:20]}", 'failed')
            else:
                print(f"\n{self.display.colored_text(f'âŒ [{domain_name}]', 'red')} çˆ¬å–å¼‚å¸¸: {e}")
            return False, None
        finally:
            # ç¡®ä¿å…³é—­çº¿ç¨‹ä¸“ç”¨çš„WebDriver
            if thread_driver:
                try:
                    thread_driver.quit()
                except:
                    pass
    
    def crawl_novel_single_domain(self, keyword, domain, search_results, max_chapters=None, use_multi_progress=False):
        """çˆ¬å–å•ä¸ªåŸŸåçš„å°è¯´
        
        Args:
            keyword (str): æœç´¢å…³é”®å­—
            domain (str): åŸŸå
            search_results (list): æœç´¢ç»“æœåˆ—è¡¨
            max_chapters (int, optional): æœ€å¤§ç« èŠ‚æ•°é™åˆ¶
            use_multi_progress (bool): æ˜¯å¦ä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
            
        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, å°è¯´æ ‡é¢˜)
        """
        domain_name = self.get_domain_name(domain)
        
        try:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
            first_result = search_results[0]
            book_url = first_result.get('url_list')
            book_title = first_result.get('articlename', keyword)
            
            # å¦‚æœä¸ä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤ºï¼Œåˆ™ä½¿ç”¨åŸæœ‰çš„æ˜¾ç¤ºæ–¹å¼
            if not use_multi_progress:
                self.display.print_domain_start(domain_name, book_title)
            
            # è·å–ç« èŠ‚åˆ—è¡¨
            chapters = self.get_chapter_list(domain, book_url)
            if not chapters:
                if use_multi_progress:
                    self.display.finish_domain_progress(domain, 0, 0)
                else:
                    self.display.print_domain_summary(domain_name, 0, 0)
                return False, None
            
            # é™åˆ¶ç« èŠ‚æ•°é‡
            if max_chapters and len(chapters) > max_chapters:
                chapters = chapters[:max_chapters]
                if not use_multi_progress:
                    print(f"  ğŸ“Š é™åˆ¶çˆ¬å–å‰ {max_chapters} ç« ï¼ˆå…± {len(chapters)} ç« å¯ç”¨ï¼‰")
            
            # åˆå§‹åŒ–è¿›åº¦ï¼ˆå¦‚æœä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤ºï¼‰
            if use_multi_progress:
                self.display.update_domain_progress(domain, 0, len(chapters), "å¼€å§‹çˆ¬å–...", 'running')
            
            # çˆ¬å–ç« èŠ‚å†…å®¹
            chapters_data = []
            for i, chapter in enumerate(chapters, 1):
                if use_multi_progress:
                    # æ›´æ–°å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
                    self.display.update_domain_progress(domain, i, len(chapters), chapter['title'], 'running')
                else:
                    # ä½¿ç”¨åŸæœ‰çš„è¿›åº¦æ˜¾ç¤º
                    self.display.print_progress(i, len(chapters), chapter['title'], domain_name)
                
                title, content = self.get_chapter_content(domain, chapter['url'])
                
                # æ ¹æ®ç»“æœå¤„ç†
                if title and content and len(content) > 50:
                    chapters_data.append((title, content))
                    if not use_multi_progress:
                        self.display.print_chapter_success(domain_name, len(content))
                else:
                    if not use_multi_progress:
                        reason = "å†…å®¹ä¸ºç©º" if not content else f"å†…å®¹è¿‡çŸ­({len(content) if content else 0}å­—)"
                        self.display.print_chapter_failed(domain_name, reason)
                
                # æ·»åŠ å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.8)  # å‡å°‘å»¶æ—¶ä»¥æé«˜å¹¶å‘æ•ˆç‡
            
            # å®Œæˆè¿›åº¦æ˜¾ç¤º
            if use_multi_progress:
                self.display.finish_domain_progress(domain, len(chapters_data), len(chapters))
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            if chapters_data:
                self.save_novel_by_domain(book_title, domain, chapters_data)
                if not use_multi_progress:
                    self.display.print_domain_summary(domain_name, len(chapters_data), len(chapters))
                return True, book_title
            else:
                if not use_multi_progress:
                    self.display.print_domain_summary(domain_name, 0, len(chapters))
                return False, None
                
        except Exception as e:
            if use_multi_progress:
                self.display.update_domain_progress(domain, 0, 0, f"é”™è¯¯: {str(e)[:20]}", 'failed')
            else:
                print(f"\n{self.display.colored_text(f'âŒ [{domain_name}]', 'red')} çˆ¬å–å¼‚å¸¸: {e}")
            return False, None
    
    def crawl_novel(self, keyword, max_chapters=None, max_workers=2):
        """çˆ¬å–å°è¯´ä¸»å‡½æ•°ï¼ˆæ”¯æŒå¤šåŸŸåï¼‰
        
        Args:
            keyword (str): æœç´¢å…³é”®å­—
            max_chapters (int, optional): æœ€å¤§ç« èŠ‚æ•°é™åˆ¶
            max_workers (int): æœ€å¤§å¹¶å‘æ•°
        """
        # ç¾åŒ–æ˜¾ç¤ºï¼šå¼€å§‹çˆ¬å–
        self.display.print_title(f"ğŸ” å¼€å§‹æœç´¢å°è¯´: {keyword}")
        mode = 'Seleniumæ¨¡å¼' if self.use_selenium else 'æ™®é€šæ¨¡å¼'
        print(f"  ğŸ”§ ä½¿ç”¨æ¨¡å¼: {self.display.colored_text(mode, 'cyan')}")
        
        # 1. åœ¨æ‰€æœ‰åŸŸåä¸­æœç´¢å°è¯´
        print(f"\n{self.display.colored_text('ğŸ“¡ æœç´¢é˜¶æ®µ', 'yellow')}")
        print("â”€" * 50)
        
        all_search_results = self.search_novel_all_domains(keyword)
        if not all_search_results:
            print(f"\n{self.display.colored_text('âŒ æœç´¢å¤±è´¥', 'red')} - æ‰€æœ‰åŸŸåéƒ½æœªæ‰¾åˆ°ç›¸å…³å°è¯´")
            return
        
        print(f"\n{self.display.colored_text('âœ… æœç´¢å®Œæˆ', 'green')} - åœ¨ {len(all_search_results)} ä¸ªåŸŸåä¸­æ‰¾åˆ°ç»“æœ")
        
        # 2. å¦‚æœä½¿ç”¨Seleniumï¼Œä¸ºäº†é¿å…å¤šçº¿ç¨‹å†²çªï¼Œæ”¹ä¸ºä¸²è¡Œå¤„ç†
        print(f"\n{self.display.colored_text('ğŸ“š çˆ¬å–é˜¶æ®µ', 'yellow')}")
        print("â”€" * 50)
        
        successful_domains = []
        novel_titles = set()  # æ”¶é›†æ‰€æœ‰å°è¯´åç§°
        
        # å¹¶è¡Œçˆ¬å–æ‰€æœ‰åŸŸåçš„å†…å®¹ï¼Œä½¿ç”¨å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
        print(f"  âš¡ å¼€å§‹å¹¶è¡Œçˆ¬å–ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰...")
        
        # åˆå§‹åŒ–å¤šåŸŸåè¿›åº¦æ˜¾ç¤º
        domains_list = list(all_search_results.keys())
        self.display.init_multi_domain_progress(domains_list)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨Seleniumé€‰æ‹©ä¸åŒçš„çˆ¬å–æ–¹æ³•
            if self.use_selenium:
                # ä½¿ç”¨æ”¯æŒå¹¶è¡Œçš„Seleniumæ–¹æ³•
                future_to_domain = {
                    executor.submit(self.crawl_novel_single_domain_with_selenium, keyword, domain, search_results, max_chapters, True): domain
                    for domain, search_results in all_search_results.items()
                }
            else:
                # ä½¿ç”¨æ™®é€šçš„å¹¶è¡Œæ–¹æ³•
                future_to_domain = {
                    executor.submit(self.crawl_novel_single_domain, keyword, domain, search_results, max_chapters, True): domain
                    for domain, search_results in all_search_results.items()
                }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_domain):
                domain = future_to_domain[future]
                try:
                    success, novel_title = future.result()
                    if success and novel_title:
                        successful_domains.append(domain)
                        novel_titles.add(novel_title)
                except Exception as e:
                    domain_name = self.get_domain_name(domain)
                    self.display.update_domain_progress(domain, 0, 0, f"å¼‚å¸¸: {str(e)[:20]}", 'failed')
        
        # å®Œæˆè¿›åº¦æ˜¾ç¤º
        self.display.finalize_progress_display()
        
        # æ˜¾ç¤ºçˆ¬å–æ€»ç»“
        self.display.print_crawl_summary(len(successful_domains))
        if successful_domains:
            for domain in successful_domains:
                print(f"    âœ… {self.display.colored_text(self.get_domain_name(domain), 'green')}")
        
        # 3. è¿›è¡Œç« èŠ‚å†…å®¹æ¯”å¯¹
        if len(successful_domains) >= 2 and novel_titles:
            self.display.print_compare_start()
            for novel_title in novel_titles:
                print(f"  ğŸ“– æ¯”å¯¹å°è¯´: {self.display.colored_text(novel_title, 'blue')}")
                self.compare_chapters(novel_title)
                
                # 4. åˆæˆæœ€ä½³ç‰ˆæœ¬
                self.display.print_merge_start()
                print(f"  ğŸ“ åˆæˆå°è¯´: {self.display.colored_text(novel_title, 'blue')}")
                self.merge_best_content(novel_title)
        else:
            print(f"\n{self.display.colored_text('â„¹ï¸  æç¤º', 'yellow')} - åªæœ‰ä¸€ä¸ªåŸŸåæˆåŠŸï¼Œæ— æ³•è¿›è¡Œå†…å®¹æ¯”å¯¹")
        
        # 5. æ¸…ç†èµ„æº
        self.cleanup()
    
    def merge_best_content(self, novel_title):
        """åˆæˆæœ€ä½³ç‰ˆæœ¬çš„å°è¯´å†…å®¹
        
        Args:
            novel_title (str): å°è¯´æ ‡é¢˜
        """
        try:
            novel_dir = os.path.join(self.output_dir, novel_title)
            if not os.path.exists(novel_dir):
                print(f"å°è¯´ç›®å½•ä¸å­˜åœ¨: {novel_dir}")
                return
            
            # è¯»å–æ¯”å¯¹æŠ¥å‘Š
            comparison_file = os.path.join(novel_dir, 'comparison_report.json')
            if not os.path.exists(comparison_file):
                print("æ¯”å¯¹æŠ¥å‘Šä¸å­˜åœ¨ï¼Œæ— æ³•åˆæˆæœ€ä½³ç‰ˆæœ¬")
                return
            
            with open(comparison_file, 'r', encoding='utf-8') as f:
                comparison_data = json.load(f)
            
            print(f"æ­£åœ¨åˆ†æ {len(comparison_data)} ä¸ªç« èŠ‚çš„å†…å®¹è´¨é‡...")
            
            # æ”¶é›†æ‰€æœ‰åŸŸåçš„ç« èŠ‚æ•°æ®
            domain_chapters = {}
            for domain_name in os.listdir(novel_dir):
                domain_path = os.path.join(novel_dir, domain_name)
                if os.path.isdir(domain_path):
                    chapters_file = os.path.join(domain_path, f'{novel_title}_chapters.json')
                    if os.path.exists(chapters_file):
                        with open(chapters_file, 'r', encoding='utf-8') as f:
                            domain_chapters[domain_name] = json.load(f)
            
            if not domain_chapters:
                print("æœªæ‰¾åˆ°ä»»ä½•åŸŸåçš„ç« èŠ‚æ•°æ®")
                return
            
            print(f"æ‰¾åˆ° {len(domain_chapters)} ä¸ªåŸŸåçš„ç« èŠ‚æ•°æ®")
            
            # åˆæˆæœ€ä½³å†…å®¹
            merged_chapters = []
            chapter_stats = {'total': 0, 'merged': 0, 'skipped': 0}
            
            # è·å–ç« èŠ‚åˆ—è¡¨ï¼ˆä»¥ç¬¬ä¸€ä¸ªåŸŸåä¸ºåŸºå‡†ï¼‰
            first_domain = list(domain_chapters.keys())[0]
            base_chapters = domain_chapters[first_domain]
            
            for chapter_info in base_chapters:
                chapter_title = chapter_info['title']
                chapter_stats['total'] += 1
                
                # æ”¶é›†è¯¥ç« èŠ‚åœ¨æ‰€æœ‰åŸŸåä¸­çš„å†…å®¹
                chapter_contents = []
                for domain_name, chapters in domain_chapters.items():
                    for ch in chapters:
                        if ch['title'] == chapter_title:
                            chapter_contents.append({
                                'domain': domain_name,
                                'content': ch['content'],
                                'length': len(ch['content'])
                            })
                            break
                
                if not chapter_contents:
                    print(f"è·³è¿‡ç« èŠ‚: {chapter_title} (æœªæ‰¾åˆ°å†…å®¹)")
                    chapter_stats['skipped'] += 1
                    continue
                
                # é€‰æ‹©æœ€ä½³å†…å®¹ï¼ˆä½¿ç”¨é«˜çº§åˆå¹¶ç­–ç•¥ï¼‰
                best_content = self._select_best_chapter_content(chapter_title, chapter_contents, comparison_data, self.reference_sources)
                
                if best_content:
                    merged_chapters.append({
                        'title': chapter_title,
                        'content': best_content['content'],
                        'source_domain': best_content['domain'],
                        'content_length': best_content['length']
                    })
                    chapter_stats['merged'] += 1
                else:
                    print(f"è·³è¿‡ç« èŠ‚: {chapter_title} (æ— æœ‰æ•ˆå†…å®¹)")
                    chapter_stats['skipped'] += 1
            
            # ä¿å­˜åˆæˆç‰ˆæœ¬
            if merged_chapters:
                self._save_merged_novel(novel_title, merged_chapters, chapter_stats)
                print(f"âœ“ åˆæˆå®Œæˆï¼å…±å¤„ç† {chapter_stats['total']} ç« ï¼ŒæˆåŠŸåˆæˆ {chapter_stats['merged']} ç« ï¼Œè·³è¿‡ {chapter_stats['skipped']} ç« ")
            else:
                print("âœ— åˆæˆå¤±è´¥ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆç« èŠ‚å†…å®¹")
                
        except Exception as e:
            print(f"åˆæˆæœ€ä½³ç‰ˆæœ¬æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _select_best_chapter_content(self, chapter_title, chapter_contents, comparison_data, reference_sources=None):
        """é€‰æ‹©æœ€ä½³çš„ç« èŠ‚å†…å®¹ - ä½¿ç”¨é«˜çº§åˆå¹¶ç­–ç•¥
        
        Args:
            chapter_title (str): ç« èŠ‚æ ‡é¢˜
            chapter_contents (list): å„åŸŸåçš„ç« èŠ‚å†…å®¹åˆ—è¡¨
            comparison_data (dict): æ¯”å¯¹æ•°æ®
            reference_sources (list): åŸºå‡†æºåˆ—è¡¨ï¼Œä¼˜å…ˆé€‰æ‹©è¿™äº›æºçš„å†…å®¹
            
        Returns:
            dict: æœ€ä½³å†…å®¹ä¿¡æ¯ï¼ŒåŒ…å«domainã€contentã€length
        """
        if not chapter_contents:
            return None
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå†…å®¹ï¼Œæ¸…ç†åè¿”å›
        if len(chapter_contents) == 1:
            cleaned_content = self._clean_chapter_content(chapter_contents[0]['content'])
            return {
                'domain': chapter_contents[0]['domain'],
                'content': cleaned_content,
                'length': len(cleaned_content)
            }
        
        # è¿‡æ»¤æ‰æ˜æ˜¾é”™è¯¯çš„å†…å®¹ï¼ˆå¤ªçŸ­æˆ–åŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
        valid_contents = []
        for content_info in chapter_contents:
            content = content_info['content']
            # è¿‡æ»¤æ¡ä»¶ï¼šå†…å®¹é•¿åº¦å¤§äºé…ç½®çš„æœ€å°é•¿åº¦ï¼Œä¸åŒ…å«å¸¸è§é”™è¯¯ä¿¡æ¯
            if (len(content) > self.merge_config['min_content_length'] and 
                'åŠ è½½ä¸­' not in content and 
                'é¡µé¢ä¸å­˜åœ¨' not in content and 
                'ç« èŠ‚ä¸å­˜åœ¨' not in content and
                'æ­£åœ¨åŠ è½½' not in content):
                valid_contents.append(content_info)
        
        if not valid_contents:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå†…å®¹ï¼Œé€‰æ‹©æœ€é•¿çš„å¹¶æ¸…ç†
            best = max(chapter_contents, key=lambda x: x['length'])
            cleaned_content = self._clean_chapter_content(best['content'])
            return {
                'domain': best['domain'],
                'content': cleaned_content,
                'length': len(cleaned_content)
            }
        
        # ç­–ç•¥1: åŸºå‡†æºä¼˜å…ˆ
        if reference_sources:
            for ref_source in reference_sources:
                for content_info in valid_contents:
                    if ref_source in content_info['domain'] and content_info['length'] > self.merge_config['min_content_length']:
                        cleaned_content = self._clean_chapter_content(content_info['content'])
                        return {
                            'domain': content_info['domain'],
                            'content': cleaned_content,
                            'length': len(cleaned_content)
                        }
        
        # ç­–ç•¥2: é•¿åº¦ä¼˜å…ˆ + è´¨é‡è¯„ä¼°
        best_content = self._select_by_length_and_quality(valid_contents)
        
        # ç­–ç•¥3: å¦‚æœæœ‰æ¯”å¯¹æ•°æ®ï¼Œä¼˜å…ˆé€‰æ‹©ç›¸ä¼¼åº¦é«˜çš„åŸŸåå†…å®¹
        if chapter_title in comparison_data:
            chapter_comparison = comparison_data[chapter_title]
            domain_scores = {}
            
            # è®¡ç®—æ¯ä¸ªåŸŸåçš„å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†
            for domain_name in [c['domain'] for c in valid_contents]:
                scores = []
                for comparison in chapter_comparison.get('comparisons', []):
                    if domain_name in comparison['domains']:
                        scores.append(comparison['similarity'])
                
                if scores:
                    domain_scores[domain_name] = sum(scores) / len(scores)
            
            # å¦‚æœæœ‰ç›¸ä¼¼åº¦æ•°æ®ï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜ä¸”å†…å®¹è¾ƒé•¿çš„
            if domain_scores:
                # ç»¼åˆè€ƒè™‘ç›¸ä¼¼åº¦å’Œå†…å®¹é•¿åº¦
                scored_contents = []
                for content_info in valid_contents:
                    domain = content_info['domain']
                    similarity_score = domain_scores.get(domain, 0.5)
                    length_score = min(content_info['length'] / 3000, 1.0)  # æ ‡å‡†åŒ–é•¿åº¦å¾—åˆ†
                    combined_score = similarity_score * 0.7 + length_score * 0.3
                    scored_contents.append((combined_score, content_info))
                
                # é€‰æ‹©ç»¼åˆå¾—åˆ†æœ€é«˜çš„
                best_content = max(scored_contents, key=lambda x: x[0])[1]
        
        # ç­–ç•¥4: å·®åˆ†ç®—æ³•åˆå¹¶ï¼ˆå¦‚æœæœ‰å¤šä¸ªç›¸ä¼¼å†…å®¹ä¸”å¯ç”¨äº†å·®åˆ†åˆå¹¶ï¼‰
        if len(valid_contents) > 1 and self.merge_config['enable_diff_merge']:
            merged_content = self._merge_with_diff_algorithm(valid_contents, best_content)
            if merged_content and len(merged_content) > len(best_content['content']):
                return {
                    'domain': 'merged',
                    'content': merged_content,
                    'length': len(merged_content)
                }
        
        # æ¸…ç†é€‰ä¸­çš„æœ€ä½³å†…å®¹
        cleaned_content = self._clean_chapter_content(best_content['content'])
        return {
            'domain': best_content['domain'],
            'content': cleaned_content,
            'length': len(cleaned_content)
        }
    
    def _clean_chapter_content(self, content):
        """æ¸…ç†ç« èŠ‚å†…å®¹ä¸­çš„å¹¿å‘Šä¿¡æ¯
        
        Args:
            content (str): åŸå§‹ç« èŠ‚å†…å®¹
            
        Returns:
            str: æ¸…ç†åçš„ç« èŠ‚å†…å®¹
        """
        import re
        
        if not content:
            return content
        
        # å®šä¹‰éœ€è¦æ¸…ç†çš„å¹¿å‘Šæ¨¡å¼
        ad_patterns = [
            # æ”¶è—æœ¬ç«™ç›¸å…³
            r'è¯·æ”¶è—æœ¬ç«™ï¼š[^\n]*',
            r'æ”¶è—æœ¬ç«™ï¼š[^\n]*',
            r'æœ¬ç«™åœ°å€ï¼š[^\n]*',
            r'æ”¶è—ç½‘å€ï¼š[^\n]*',
            
            # ç¬”è¶£é˜æ‰‹æœºç‰ˆç›¸å…³
            r'ç¬”è¶£é˜æ‰‹æœºç‰ˆï¼š[^\n]*',
            r'æ‰‹æœºç‰ˆï¼š[^\n]*',
            r'æ‰‹æœºç«™ï¼š[^\n]*',
            r'ç§»åŠ¨ç‰ˆï¼š[^\n]*',
            
            # æ¥æºç«™ç‚¹ä¿¡æ¯
            r'æ¥æºäº[^\n]*ç«™[^\n]*',
            r'æœ¬ç« æ¥è‡ª[^\n]*',
            r'è½¬è½½è¯·æ³¨æ˜[^\n]*',
            r'æ›´æ–°æœ€å¿«[^\n]*',
            r'\(æ¥æº:.*?\)',  # æ¸…ç†æ¥æºæ ‡è®°
            
            # ç‚¹å‡»ç›¸å…³
            r'ã€ç‚¹æ­¤æŠ¥é”™ã€[^\n]*',
            r'ã€åŠ å…¥ä¹¦ç­¾ã€[^\n]*',
            r'ã€ç‚¹å‡»æŠ¥é”™ã€[^\n]*',
            r'ã€ç‚¹å‡»æ”¶è—ã€[^\n]*',
            r'\[ç‚¹æ­¤æŠ¥é”™\][^\n]*',
            r'\[åŠ å…¥ä¹¦ç­¾\][^\n]*',
            
            # ç½‘å€é“¾æ¥
            r'https?://[^\s\n]*',
            r'www\.[^\s\n]*',
            r'm\.[^\s\n]*\.(?:com|net|org|cn|cc)[^\s\n]*',
            
            # å¨ä¿¡å¹³å°æ¨å¹¿ç›¸å…³
            r'å¨ä¿¡.*?å¹³å°.*?æ–¹æ³•.*?',  # æ¸…ç†å¨ä¿¡å¹³å°æ¨å¹¿
            r'è…¾è®¯å¨åš.*?æ‰«æ.*?',  # æ¸…ç†è…¾è®¯å¨åšæ¨å¹¿
            r'é€šè®¯å½•.*?æŸ¥æ‰¾.*?',  # æ¸…ç†é€šè®¯å½•ç›¸å…³
            r'æœå¯».*?éªŒè¯æ ‡è®°.*?',  # æ¸…ç†æœå¯»éªŒè¯ç›¸å…³
            r'wang--yu----.*?',  # æ¸…ç†å…·ä½“å¨ä¿¡å·
            r'å¿˜è¯­.*?å¨ä¿¡.*?',  # æ¸…ç†ä½œè€…å¨ä¿¡æ¨å¹¿
            r'å®ä½“ç­¾åä¹¦.*?',  # æ¸…ç†ç­¾åä¹¦æ¨å¹¿
            r'ç¥ç§˜å¤§å¥–.*?',  # æ¸…ç†å¥–å“æ¨å¹¿
            r'\(.*?å¨ä¿¡.*?\)',  # æ¸…ç†æ‹¬å·å†…å¨ä¿¡ç›¸å…³å†…å®¹
            r'å¹³å°ä¸‹é¢.*?å…¬ä¼—å·.*?',  # æ¸…ç†å¹³å°å…¬ä¼—å·æ¨å¹¿
            r'ç­‰.*?æ­£æ–‡å¼€å§‹ä¸Šä¼ .*?',  # æ¸…ç†ä¸Šä¼ ç›¸å…³æ¨å¹¿
            r'\(.*?\)\s*\(',  # æ¸…ç†è¿ç»­çš„æ‹¬å·å†…å®¹
            r'----.*?',  # æ¸…ç†ç ´æŠ˜å·åçš„å†…å®¹
            
            # å…¶ä»–å¹¿å‘Šä¿¡æ¯
            r'\(æœªå®Œå¾…ç»­[^\)]*\)',
            r'æœªå®Œå¾…ç»­[^\n]*',
            r'æœ€æ–°ç« èŠ‚[^\n]*',
            r'æ›´æ–°æ—¶é—´[^\n]*',
            r'å­—æ•°ç»Ÿè®¡[^\n]*',
            r'é˜…è¯»æç¤º[^\n]*',
            
            # ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç 
            r'&[a-zA-Z]+;',  # HTMLå®ä½“
            r'\\u[0-9a-fA-F]{4}',  # Unicodeç¼–ç 
            
            # å¤šä½™çš„ç©ºè¡Œï¼ˆ3ä¸ªä»¥ä¸Šè¿ç»­æ¢è¡Œç¬¦ï¼‰
            r'\n{3,}',
        ]
        
        cleaned_content = content
        
        # é€ä¸ªåº”ç”¨æ¸…ç†æ¨¡å¼
        for pattern in ad_patterns:
            cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned_content = re.sub(r'[ \t]+', ' ', cleaned_content)  # å¤šä¸ªç©ºæ ¼/åˆ¶è¡¨ç¬¦åˆå¹¶ä¸ºä¸€ä¸ªç©ºæ ¼
        cleaned_content = re.sub(r'\n[ \t]+', '\n', cleaned_content)  # è¡Œé¦–ç©ºç™½å­—ç¬¦
        cleaned_content = re.sub(r'[ \t]+\n', '\n', cleaned_content)  # è¡Œå°¾ç©ºç™½å­—ç¬¦
        cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)  # å¤šä¸ªæ¢è¡Œç¬¦åˆå¹¶ä¸ºä¸¤ä¸ª
        
        # æ¸…ç†å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦
        cleaned_content = cleaned_content.strip()
        
        return cleaned_content
    
    def _select_by_length_and_quality(self, valid_contents):
        """åŸºäºé•¿åº¦å’Œè´¨é‡é€‰æ‹©æœ€ä½³å†…å®¹
        
        Args:
            valid_contents (list): æœ‰æ•ˆå†…å®¹åˆ—è¡¨
            
        Returns:
            dict: æœ€ä½³å†…å®¹ä¿¡æ¯
        """
        best_content = None
        best_score = -1
        
        for content_info in valid_contents:
            content = content_info['content']
            
            # è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°ï¼ˆåŸºäºé•¿åº¦å’Œå†…å®¹ç‰¹å¾ï¼‰
            length_score = min(len(content) / 2000, 1.0)  # é•¿åº¦åˆ†æ•°ï¼Œæœ€å¤§1.0
            
            # ç®€å•çš„è´¨é‡è¯„ä¼°
            quality_score = 0.5  # åŸºç¡€åˆ†æ•°
            if len(content) > 1000:  # å†…å®¹è¾ƒé•¿
                quality_score += 0.3
            if 'ç« ' in content or 'èŠ‚' in content:  # åŒ…å«ç« èŠ‚æ ‡è¯†
                quality_score += 0.1
            if content.count('\n') > 5:  # æœ‰åˆç†çš„æ®µè½åˆ†å¸ƒ
                quality_score += 0.1
            
            # ä½¿ç”¨é…ç½®çš„æƒé‡è®¡ç®—æ€»åˆ†
            total_score = (length_score * self.merge_config['length_priority_weight'] + 
                          quality_score * self.merge_config['quality_weight'])
            
            if total_score > best_score:
                best_score = total_score
                best_content = content_info
                
        return best_content or valid_contents[0]
    
    def _merge_with_diff_algorithm(self, valid_contents, base_content):
        """ä½¿ç”¨å·®åˆ†ç®—æ³•åˆå¹¶å¤šä¸ªå†…å®¹ç‰ˆæœ¬
        
        Args:
            valid_contents (list): æœ‰æ•ˆå†…å®¹åˆ—è¡¨
            base_content (dict): åŸºå‡†å†…å®¹
            
        Returns:
            str: åˆå¹¶åçš„å†…å®¹
        """
        try:
            import difflib
            
            base_text = base_content['content']
            base_lines = base_text.splitlines()
            
            # æ”¶é›†æ‰€æœ‰ä¸åŒç‰ˆæœ¬çš„è¡Œ
            all_lines = set(base_lines)
            for content_info in valid_contents:
                if content_info['domain'] != base_content['domain']:
                    lines = content_info['content'].splitlines()
                    all_lines.update(lines)
            
            # ä½¿ç”¨æœ€é•¿çš„å†…å®¹ä½œä¸ºåŸºç¡€ï¼Œè¡¥å……ç¼ºå¤±çš„è¡Œ
            longest_content = max(valid_contents, key=lambda x: x['length'])
            longest_lines = longest_content['content'].splitlines()
            
            # ç®€å•çš„åˆå¹¶ç­–ç•¥ï¼šä¿æŒæœ€é•¿ç‰ˆæœ¬çš„ç»“æ„ï¼Œè¡¥å……æœ‰ä»·å€¼çš„è¡Œ
            merged_lines = longest_lines.copy()
            
            # æŸ¥æ‰¾å…¶ä»–ç‰ˆæœ¬ä¸­å¯èƒ½ç¼ºå¤±çš„é‡è¦å†…å®¹
            for content_info in valid_contents:
                if content_info['domain'] != longest_content['domain']:
                    other_lines = content_info['content'].splitlines()
                    
                    # æŸ¥æ‰¾åœ¨æœ€é•¿ç‰ˆæœ¬ä¸­ä¸å­˜åœ¨ä½†åœ¨å…¶ä»–ç‰ˆæœ¬ä¸­å­˜åœ¨çš„æœ‰ä»·å€¼è¡Œ
                    for line in other_lines:
                        line = line.strip()
                        if (len(line) > 10 and  # è¡Œé•¿åº¦è¶³å¤Ÿ
                            line not in '\n'.join(merged_lines) and  # ä¸é‡å¤
                            not any(ad in line.lower() for ad in ['å¹¿å‘Š', 'æ¨å¹¿', 'æ”¶è—', 'ç‚¹å‡»']) and  # ä¸æ˜¯å¹¿å‘Š
                            any(char in line for char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')):
                            # åœ¨é€‚å½“ä½ç½®æ’å…¥è¿™è¡Œå†…å®¹
                            merged_lines.append(line)
            
            merged_text = '\n'.join(merged_lines)
            
            # å¦‚æœåˆå¹¶åçš„å†…å®¹æ˜æ˜¾æ›´é•¿ä¸”è´¨é‡æ›´å¥½ï¼Œè¿”å›åˆå¹¶ç»“æœ
            if len(merged_text) > len(base_text) * self.merge_config['merge_threshold']:
                return self._clean_chapter_content(merged_text)
            
            return None
            
        except Exception as e:
            print(f"å·®åˆ†åˆå¹¶å¤±è´¥: {e}")
            return None
    
    def _save_merged_novel(self, novel_title, merged_chapters, stats):
        """ä¿å­˜åˆæˆçš„å°è¯´
        
        Args:
            novel_title (str): å°è¯´æ ‡é¢˜
            merged_chapters (list): åˆæˆçš„ç« èŠ‚åˆ—è¡¨
            stats (dict): ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            novel_dir = os.path.join(self.output_dir, novel_title)
            
            # åˆ›å»ºåˆæˆç‰ˆæœ¬ç›®å½•
            merged_dir = os.path.join(novel_dir, 'merged_best')
            if not os.path.exists(merged_dir):
                os.makedirs(merged_dir)
            
            # ä¿å­˜åˆæˆçš„txtæ–‡ä»¶
            txt_file = os.path.join(merged_dir, f'{novel_title}_merged.txt')
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(f"å°è¯´åç§°: {novel_title}\n")
                f.write(f"åˆæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ç« èŠ‚æ€»æ•°: {stats['merged']}\n")
                f.write("=" * 50 + "\n\n")
                
                for i, chapter in enumerate(merged_chapters, 1):
                    f.write(f"ç¬¬{i}ç«  {chapter['title']}\n")
                    f.write(f"(æ¥æº: {chapter['source_domain']}, é•¿åº¦: {chapter['content_length']})\n")
                    f.write("-" * 30 + "\n")
                    f.write(chapter['content'])
                    f.write("\n\n")
            
            # ä¿å­˜åˆæˆä¿¡æ¯çš„JSONæ–‡ä»¶
            json_file = os.path.join(merged_dir, f'{novel_title}_merged_info.json')
            merged_info = {
                'novel_title': novel_title,
                'merge_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'statistics': stats,
                'chapters': [{
                    'index': i + 1,
                    'title': chapter['title'],
                    'source_domain': chapter['source_domain'],
                    'content_length': chapter['content_length']
                } for i, chapter in enumerate(merged_chapters)]
            }
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(merged_info, f, ensure_ascii=False, indent=2)
            
            print(f"åˆæˆç‰ˆæœ¬å·²ä¿å­˜åˆ°: {merged_dir}")
            print(f"  - æ–‡æœ¬æ–‡ä»¶: {txt_file}")
            print(f"  - ä¿¡æ¯æ–‡ä»¶: {json_file}")
            
        except Exception as e:
            print(f"ä¿å­˜åˆæˆç‰ˆæœ¬æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å°è¯´çˆ¬è™«è„šæœ¬ï¼ˆæ”¯æŒå¤šåŸŸåå¹¶è¡Œçˆ¬å–å’Œå†…å®¹æ¯”å¯¹ï¼‰')
    parser.add_argument('keyword', help='æœç´¢å…³é”®å­—')
    parser.add_argument('--max-chapters', type=int, help='æœ€å¤§ç« èŠ‚æ•°é™åˆ¶')
    parser.add_argument('--domains-file', default='all_domains.json', help='åŸŸåé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default='novel_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-workers', type=int, default=2, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--compare-only', action='store_true', help='ä»…è¿›è¡Œå†…å®¹æ¯”å¯¹ï¼ˆä¸çˆ¬å–æ–°å†…å®¹ï¼‰')
    parser.add_argument('--use-selenium', action='store_true', help='ä½¿ç”¨Seleniumå¤„ç†JavaScript')
    
    args = parser.parse_args()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = NovelCrawler(args.domains_file, args.output_dir, args.use_selenium)
    
    if args.compare_only:
        # ä»…è¿›è¡Œå†…å®¹æ¯”å¯¹
        print(f"å¼€å§‹æ¯”å¯¹å°è¯´å†…å®¹...")
        # æŸ¥æ‰¾è¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰å°è¯´
        if os.path.exists(args.output_dir):
            for novel_name in os.listdir(args.output_dir):
                novel_path = os.path.join(args.output_dir, novel_name)
                if os.path.isdir(novel_path):
                    print(f"æ¯”å¯¹å°è¯´: {novel_name}")
                    crawler.compare_chapters(novel_name)
        else:
            print(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {args.output_dir}")
    else:
        # å¼€å§‹çˆ¬å–
        crawler.crawl_novel(args.keyword, args.max_chapters, args.max_workers)

if __name__ == '__main__':
    main()